---
title: "p8105_hw6_hc3212"
author: "Hening CUi"
date: "12/1/2021"
output: github_document
---

```{r,echo = FALSE}
library (tidyverse)
library(viridis)
library(modelr)
library(mgcv)
library(MASS)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

knitr::opts_chunk$set(
  fig.width = 8,
  fig.height =6,
  out.width = "90%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

First import and clean the data.

```{r import}
baby_df = 
  read_csv("birthweight.csv") %>% 
  drop_na() %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    mrace = as.factor(mrace),
  ) %>% 
  dplyr::select(-pnumlbw, -pnumsga)
```

I use the backwards elimination to creat the optimal linear regression. Firstly, all the predictor was included to generate the linear regression. Then the less significant predictor was removed, which p-value less than SL(0.05). The final model contains babysex, bhead, blength, delwt, fincome, gaweeks, mheight, mrace, parity, ppwt and smoken.

```{r fit_model}
mult.fit = lm(bwt ~ ., data = baby_df)
summary(mult.fit)

final.model <- stepAIC(mult.fit, direction = "both", trace = FALSE) 
summary(final.model)
```


```{r residue_vs_fit}
baby_df%>% 
  add_predictions(final.model) %>% 
  add_residuals(final.model) %>% 
  ggplot(aes(x = pred, y = resid, alpha = 0.1)) +
  geom_point()
```

First, sampling...

```{r sampling}
baby_cv =
  crossv_mc(baby_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

Next, Fit models

```{r compare_fit}
fit_df =
  baby_cv %>% 
  mutate(
    final_mod = map(.x = train, ~stepAIC(lm(bwt ~ ., data = .x), direction = "both", trace = FALSE)),
    len_mod = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    hea_mod = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    rmsefinal = map2_dbl(.x = final_mod,.y = test, ~rmse(model = .x, data = .y)),
    rmselen = map2_dbl(.x = len_mod,.y = test, ~rmse(model = .x, data = .y)),
    rmsehea = map2_dbl(.x = hea_mod,.y = test, ~rmse(model = .x, data = .y))
  )

```

Draw the plot

```{r boxplot}
fit_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    rmsefinal:rmsehea,
    names_to = "model",
    values_to = "rmse", 
    names_prefix = "rmse"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot() +
  scale_x_discrete(labels = c("backwards", "length and gestational", "head, length, and sex"))
```

## Problem 2

```{r import_weather}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  dplyr::select(name, id, everything())
```

let's bootstrap

```{r bootstrap}
weather_boot =
  weather_df %>% 
  drop_na() %>% 
  bootstrap(5000, id = "strap_number")%>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy),
    a = map(models, broom::glance)
  ) %>% 
  dplyr::select(strap_number, results, a) %>% 
  unnest(results, a)
```

Calculate $log(\beta_{0} * \beta_{1})$

```{r callog}
callog =
  weather_boot %>% 
  dplyr::select(strap_number, term, estimate) %>% 
  pivot_wider(names_from = term,
              values_from = estimate) %>% 
  rename(intercept ="(Intercept)") %>% 
  mutate(logBB = log(intercept*tmin))
```

Plot the distribution of $log(\beta_{0} * \beta_{1})$

```{r densi_BB}
callog %>% 
  ggplot(aes(x = logBB)) +
  geom_density() + 
  xlab("log(B0 * B1)") +
  ggtitle("Distribution of log(B0 * B1)")
```

It could find from the figure that the distribution of $log(\beta_{0} * \beta_{1})$ is approximately normal, when $log(\beta_{0} * \beta_{1})$ around 2.02, it has maximum density. The distribution is little right-skewed, which may be better with higher bootstrap times.


Calculate the 95% CI

```{r CI_BB}
callog %>% 
  summarize(
    ci_lower = quantile(logBB, 0.025),
    ci_upper = quantile(logBB,0.975)
  ) %>% 
  knitr::kable()
```

Calculate $r^2$

```{r r_square}
r_square =
  weather_boot %>% 
  filter(term == "tmin") %>% 
  dplyr::select(r.squared) 
```

Plot the distribution of  $r^2$

```{r densi_r}
r_square %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() + 
  xlab("r square") +
  ggtitle("Distribution of r square")
```

It could find from the figure that the distribution of  $r^2$ is approximately normal, when  $r^2$ around 0.91, it has maximum density. The distribution is little right-skewed, which may be better with higher bootstrap times.


Calculate the 95% CI

```{r CI_r}
r_square %>% 
  summarize(
    ci_lower = quantile(r.squared, 0.025),
    ci_upper = quantile(r.squared,0.975)
  ) %>% 
  knitr::kable()
```
 
 
 

