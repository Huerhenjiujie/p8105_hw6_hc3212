---
title: "p8105_hw6_hc3212"
author: "Hening CUi"
date: "12/1/2021"
output: github_document
---

```{r,echo = FALSE}
library (tidyverse)
library(viridis)
library(modelr)
library(mgcv)
library(MASS)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

knitr::opts_chunk$set(
  fig.width = 8,
  fig.height =6,
  out.width = "90%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

First import and clean the data.

```{r import}
baby_df = 
  read_csv("birthweight.csv") %>% 
  drop_na() %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    mrace = as.factor(mrace),
  ) %>% 
  dplyr::select(-pnumlbw, -pnumsga)
```

I use the backwards elimination to creat the optimal linear regression. Firstly, all the predictor was included to generate the linear regression. Then the less significant predictor was removed, which p-value less than SL(0.05). The final model contains babysex, bhead, blength, delwt, fincome,  
gaweeks, mheight, mrace, parity, ppwt and smoken.

```{r fit_model}
mult.fit = lm(bwt ~ ., data = baby_df)
summary(mult.fit)

final.model <- stepAIC(mult.fit, direction = "both", trace = FALSE) 
summary(final.model)
```


```{r residue_vs_fit}
baby_df%>% 
  add_predictions(final.model) %>% 
  add_residuals(final.model) %>% 
  ggplot(aes(x = pred, y = resid, alpha = 0.1)) +
  geom_point()
```

First, sampling...

```{r sampling}
baby_cv =
  crossv_mc(baby_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

Next, Fit models

```{r compare_fit}
fit_df =
  baby_cv %>% 
  mutate(
    final_mod = map(.x = train, ~stepAIC(lm(bwt ~ ., data = .x), direction = "both", trace = FALSE)),
    len_mod = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    hea_mod = map(.x = train, ~lm(bwt ~ bhead + blength + babysex, data = .x))
  ) %>% 
  mutate(
    rmsefinal = map2_dbl(.x = final_mod,.y = test, ~rmse(model = .x, data = .y)),
    rmselen = map2_dbl(.x = len_mod,.y = test, ~rmse(model = .x, data = .y)),
    rmsehea = map2_dbl(.x = hea_mod,.y = test, ~rmse(model = .x, data = .y))
  )

```



```{r boxplot}
fit_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    rmsefinal:rmsehea,
    names_to = "model",
    values_to = "rmse", 
    names_prefix = "rmse"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot() +
  scale_x_discrete(labels = c("backwards", "length and gestational", "head, length, and sex"))
```

## Problem 2

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  dplyr::select(name, id, everything())
```

let's bootstrap

```{r}
weather_boot =
  weather_df %>% 
  drop_na() %>% 
  bootstrap(5000, id = "strap_number")%>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  dplyr::select(strap_number, results) %>% 
  unnest(results)
```

The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. We’ll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data:

r̂2
log(β̂0∗β̂1)
Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂2
 and log(β̂0∗β̂1)
. Note: broom::glance() is helpful for extracting r̂2
 from a fitted regression, and broom::tidy() (with some additional wrangling) should help in computing log(β̂0∗β̂1)
.



